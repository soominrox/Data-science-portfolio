
# Emojify

- ëª©í‘œ: ë‹¨ì–´ ë°±í„° ì‚¬ìš©í•´ì„œ Emojifierë¥¼ êµ¬í˜„í•˜ì

ì¹´í†¡ì„ ë³´ë‚¼ë•Œ ë‚´ìš©ì„ ë” í‘œí˜„ë ¥ìˆê²Œ ì „ë‹¬í•˜ê³  ì‹¶ì—ˆë˜ ì ì´ ë§ì§€ ì•Šì€ê°€? EmojifierëŠ” ì´ë¥¼ ë•ëŠ” ëª¨ë¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "í•©ê²© ì¶•í•˜í•´! ë‚˜ì¤‘ì— ì»¤í”¼ ë§ˆì‹œë©´ì„œ ìˆ˜ë‹¤ë–¨ì~ ì‚¬ë‘í•´!" ë¼ëŠ” ë‚´ìš©ì„ emojifierëŠ” ìë™ìœ¼ë¡œ ì´ ë¬¸ì¥ì„ "í•©ê²© ì¶•í•˜í•´!ğŸ‘ ë‚˜ì¤‘ì— ì»¤í”¼ ë§ˆì‹œë©´ì„œ ìˆ˜ë‹¤ë–¨ì~â˜•ï¸ ì‚¬ë‘í•´!â¤ï¸"ë¼ê³  ë°”ê¿”ì¤€ë‹¤.

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë¬¸ì¥ì„ ì…ë ¥í–ˆì„ë•Œ ì´ ë¬¸ì¥ ë‚´ìš©ê³¼ ê°€ì¥ ì ì ˆí•œ ì´ëª¨í‹°ì½˜ì„ ìë™ìœ¼ë¡œ ì…ë ¥í•˜ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•  ê²ƒì´ë‹¤. ë‹¨ì–´ ë²¡í„°ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ train setì—ì„œ íŠ¹ì • ì´ëª¨í‹°ì½˜ê³¼ ëª‡ ì•ˆë˜ëŠ” ë‹¨ì–´ë“¤ë§Œ ì—°ê´€ì‹œì¼œë„ ì•Œê³ ë¦¬ì¦˜ì´ ì´ë¥¼ ì¼ë°˜í™”ì‹œí‚¤ê¸° ë•Œë¬¸ì— test setì—ì„œ ìƒˆ ë‹¨ì–´ê°€ ë‚˜ì™€ë„ ì´ë¥¼ ì´ëª¨í‹°ì½˜ê³¼ ì—°ê´€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì´ëŠ” ì ì€ ì–‘ì˜ train setìœ¼ë¡œë„ ê½¤ ì •í™•í•œ classifier mappingì„ í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

ê°€ì¥ ë¨¼ì € baseline ëª¨ë¸ì„ ë§Œë“¤ì–´ word embeddingì„ í•˜ê³ , ê·¸ ë‹¤ìŒ LSTMì„ ì´ìš©í•´ ë” ì •êµí•œ ëª¨ë¸ì„ ë§Œë“¤ì.

You will implement a model which inputs a sentence (such as "Let's go see the baseball game tonight!") and finds the most appropriate emoji to be used with this sentence (âš¾ï¸). In many emoji interfaces, you need to remember that â¤ï¸ is the "heart" symbol rather than the "love" symbol. But using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. 


```python
import numpy as np
from emo_utils import *
import emoji
import matplotlib.pyplot as plt

%matplotlib inline
```

## 1 - Baseline model: Emojifier-V1

### 1.1 - Dataset EMOJISET

ê°€ì¥ ê°„ë‹¨í•œ baseline ë¶„ë¥˜ê¸°(Emojifier-V1)ë¥¼ ë§Œë“¤ì–´ë³´ì.

ë°ì´í„° êµ¬ì¡°:
- X: 127ê°œì˜ ë¬¸ì¥ì´ ë‹´ê¹€
- Y: 0~4ì˜ ì •ìˆ˜, ê° ë¬¸ì¥ì— í•´ë‹¹í•˜ëŠ” ì´ëª¨í‹°ì½˜ì„ ë ˆì´ë¸”ë§í•¨

<img src="images/data_set.png" style="width:700px;height:300px;">
<caption><center> **Figure 1**: EMOJISET - 5ê°œì˜ ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¼ í•  ìˆ˜ ìˆë‹¤. </center></caption>

### Load Dataset


```python
def read_csv(filename = 'data/emojify_data.csv'):
    phrase = []
    emoji = []

    with open (filename) as csvDataFile:
        csvReader = csv.reader(csvDataFile)

        for row in csvReader:
            phrase.append(row[0])
            emoji.append(row[1])

    X = np.asarray(phrase)
    Y = np.asarray(emoji, dtype=int)

    return X, Y
```


```python
# train: 127ê°œì˜ ìƒ˜í”Œ
X_train, Y_train = read_csv('data/train_emoji.csv')

# test: 56ê°œì˜ ìƒ˜í”Œ
X_test, Y_test = read_csv('data/tesss.csv')
```


```python
maxLen = len(max(X_train, key=len).split())
print(maxLen)
```




    10




```python
index = 7
print(X_train[index], label_to_emoji(Y_train[index]))
```

    congratulations on your acceptance ğŸ˜„


### 1.2 - Overview of the Emojifier-V1

Baseline ëª¨ë¸ "Emojifier-v1"ì„ ë§Œë“¤ì. ì‘ë™ ì›ë¦¬ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

<center>
<img src="images/image_1.png" style="width:900px;height:300px;">
<caption><center> **Figure 2**: Baseline ëª¨ë¸ (Emojifier-V1).</center></caption>
</center>

ëª¨ë¸ì˜ inputì€ ë¬¸ì¥ ì•ˆì˜ ê° ë‹¨ì–´ë“¤ì´ë‹¤. outputì€ (1,5)í˜•íƒœì˜ í™•ë¥ ë²¡í„°ë¡œ, argmax layerë¥¼ í†µê³¼ì‹œì¼œ ê°€ì¥ ì ì ˆí•œ ì´ëª¨í‹°ì½˜ì˜ ì¸ë±ìŠ¤ë¥¼ ì¶œë ¥í•˜ê²Œ í•œë‹¤.

The input of the model is a string corresponding to a sentence (e.g. "I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output.

ë ˆì´ë¸”ì„ softmax classifierì— í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ shapeì„ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤. ì¦‰, í˜„ì¬ (m,1)ì¸ Yì»¬ëŸ¼ì„ one-hot representationì¸ (m,5)ë¡œ ë°”ê¿”ì£¼ì. ê° í–‰ì€ ê° ë¬¸ì¥ì— í•´ë‹¹í•˜ëŠ” ë ˆì´ë¸”ì— ëŒ€í•œ one-hot ë²¡í„°ì´ë‹¤.

To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape  $(m, 1)$ into a "one-hot representation" $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, `Y_oh` stands for "Y-one-hot" in the variable names `Y_oh_train` and `Y_oh_test`: 



```python
def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)]
    return Y

Y_oh_train = convert_to_one_hot(Y_train, C = 5)
Y_oh_test = convert_to_one_hot(Y_test, C = 5)
```

Let's see what `convert_to_one_hot()` did. Feel free to change `index` to print out different values. 


```python
index = 7
print("ë¬¸ì¥", Y_train[index], ": one-hotë²¡í„°", Y_oh_train[index], "ë¡œ ë³€í™˜ë¨")
```

    ë¬¸ì¥ 2 : one-hotë²¡í„° [ 0.  0.  1.  0.  0.] ë¡œ ë³€í™˜ë¨


ë°ì´í„° ì „ì²˜ë¦¬ ë! ì´ì œ ëª¨ë¸ì„ êµ¬í˜„í•´ë³´ì.

### 1.3 - Implementing Emojifier-V1

As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the `word_to_vec_map`, which contains all the vector representations.

ì²« ë‹¨ê³„ëŠ” inputë¬¸ì¥ì„ word vector representationìœ¼ë¡œ ë³€í™˜í•œ í›„ í‰ê· ì„ ë‚´ëŠ” ê²ƒì´ë‹¤. Pretrainëœ 50-dimensionì˜ GloVe embeddingì„ ì‚¬ìš©í•˜ì. 


```python
word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')
```

- `word_to_index`: (ë‹¨ì–´ -> ì¸ë±ìŠ¤)ë¡œ ë”•ì…”ë„ˆë¦¬ ë§µí•‘
(400,001 words, with the valid indices ranging from 0 to 400,000)
- `index_to_word`: (ì¸ë±ìŠ¤ -> ë‹¨ì–´)ë¡œ ë”•ì…”ë„ˆë¦¬ ë§µí•‘
- `word_to_vec_map`: (ë‹¨ì–´ -> GloVeë²¡í„°)ë¡œ ë”•ì…”ë„ˆë¦¬ ë§µí•‘


```python
word = "cucumber"
index = 289846
print("the index of", word, "in the vocabulary is", word_to_index[word])
print("the", str(index) + "th word in the vocabulary is", index_to_word[index])
```

    the index of cucumber in the vocabulary is 113317
    the 289846th word in the vocabulary is potatos


2ë‹¨ê³„ë¡œ `sentence_to_avg()` êµ¬í˜„í•˜ê¸°:

1. ëª¨ë“  ë¬¸ì¥ì„ ì†Œë¬¸ìë¡œ ë§Œë“  í›„, ê° ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ listë¡œ ë§Œë“¤ì.
2. ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë“¤ì— í•´ë‹¹í•˜ëŠ” GloVe representationì„ êµ¬í•œ í›„ ì´ ê°’ë“¤ì„ í‰ê· ë‚´ì.


```python
def sentence_to_avg(sentence, word_to_vec_map):
    
    words = sentence.lower().split()

    #í‰ê·  ë‹¨ì–´ ë²¡í„° ì´ˆê¸°í™”í•˜ê¸°
    avg = np.zeros((50,))
    
    #ë‹¨ì–´ë²¡í„° í‰ê·  êµ¬í•˜ê¸°
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg / len(words)
    
    return avg
```


```python
avg = sentence_to_avg("I love you", word_to_vec_map)
print("avg = ", avg)
```

    avg =  [-0.00701397  0.54196333 -0.19225433 -0.52465667  0.77827667 -0.04985667
     -0.31802333  0.1746592  -0.52433     0.47641919 -0.33454333  0.93489
     -0.61863667 -0.164186    1.10000667  0.33991333  0.29203     0.35769333
      0.07931167 -0.724163   -0.42256667  0.87212     0.70861667  0.45412333
      1.2277     -2.0613     -1.31806667  0.23561667  1.2105     -1.26068
      3.33396667  0.74604667 -0.60947     0.23688667 -0.31138667 -0.179042
      0.17087667  0.119286    0.35114667 -0.56632333  0.09226223 -0.03197267
     -0.20612     0.41710333  0.168862    0.18619333  0.08125467 -0.80106333
     -0.20057967  0.78087   ]


#### Model

ìœ„ì˜ `sentence_to_avg()`ì‚¬ìš© í›„ êµ¬í•œ í‰ê· ê°’ì„ forward propagation í•˜ê³  costë¥¼ êµ¬í•˜ì. ê·¸ëŸ° ë‹¤ìŒ backpropagateí•´ì„œ softmaxì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì. ì‚¬ìš©í•  ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ z^{(i)} = W . avg^{(i)} + b$$
$$ a^{(i)} = softmax(z^{(i)})$$
$$ \mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$



```python
def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()
```


```python
def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):

    np.random.seed(1)

    m = Y.shape[0]                          # train set ìƒ˜í”Œ ìˆ˜
    n_y = 5                                 # í´ë˜ìŠ¤ ê°œìˆ˜  
    n_h = 50                                # GloVe vectors í¬ê¸° 
    
    # Xavier initializationìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
    b = np.zeros((n_y,))
    
    # Y ë¥¼ n_yê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ Y_onehot ìœ¼ë¡œ ë³€í™˜í•˜ê¸°
    Y_oh = convert_to_one_hot(Y, C = n_y) 
    
    # Optimization loop
    for t in range(num_iterations):  
        
        # Loop over the training examples
        for i in range(m):                                

            # ië²ˆì§¸ ìƒ˜í”Œì˜ ë‹¨ì–´ë“¤ì— ëŒ€í•œ word ë²¡í„°ë“¤ì˜ í‰ê·  êµ¬í•˜ê¸°
            avg = sentence_to_avg(X[i], word_to_vec_map)

            # í‰ê·  Forward propagate í•˜ê¸°
            z = np.dot(W, avg) + b
            a = softmax(z)

            # Cost êµ¬í•˜ê¸°
            cost = -np.sum(Y_oh[i] * np.log(a))

            # gradients êµ¬í•˜ê¸°
            dz = a - Y_oh[i]
            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
            db = dz

            # SGDë¡œ íŒŒë¼ë¯¸í„° Update 
            W = W - learning_rate * dW
            b = b - learning_rate * db
        
        if t % 100 == 0:
            print("Epoch: " + str(t) + " --- cost = " + str(cost))
            pred = predict(X, Y, W, b, word_to_vec_map)

    return pred, W, b
```

ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ softmax íŒŒë¼ë¯¸í„° W,bë¥¼ êµ¬í•˜ì.



```python
pred, W, b = model(X_train, Y_train, word_to_vec_map)
print(pred)
```

    Epoch: 0 --- cost = 1.95204988128
    Accuracy: 0.348484848485
    Epoch: 100 --- cost = 0.0797181872601
    Accuracy: 0.931818181818
    Epoch: 200 --- cost = 0.0445636924368
    Accuracy: 0.954545454545
    Epoch: 300 --- cost = 0.0343226737879
    Accuracy: 0.969696969697
    [[ 3.]
     [ 2.]
     [ 3.]
     [ 0.]
     [ 4.]
     [ 0.]
     [ 3.]
     [ 2.]
     [ 3.]
     [ 1.]
     [ 3.]
     [ 3.]
     [ 1.]
     [ 3.]
     [ 2.]
     [ 3.]
     [ 2.]
     [ 3.]
     [ 1.]
     [ 2.]
     [ 3.]
     [ 0.]
     [ 2.]
     [ 2.]
     [ 2.]
     [ 1.]
     [ 4.]
     [ 3.]
     [ 3.]
     [ 4.]
     [ 0.]
     [ 3.]
     [ 4.]
     [ 2.]
     [ 0.]
     [ 3.]
     [ 2.]
     [ 2.]
     [ 3.]
     [ 4.]
     [ 2.]
     [ 2.]
     [ 0.]
     [ 2.]
     [ 3.]
     [ 0.]
     [ 3.]
     [ 2.]
     [ 4.]
     [ 3.]
     [ 0.]
     [ 3.]
     [ 3.]
     [ 3.]
     [ 4.]
     [ 2.]
     [ 1.]
     [ 1.]
     [ 1.]
     [ 2.]
     [ 3.]
     [ 1.]
     [ 0.]
     [ 0.]
     [ 0.]
     [ 3.]
     [ 4.]
     [ 4.]
     [ 2.]
     [ 2.]
     [ 1.]
     [ 2.]
     [ 0.]
     [ 3.]
     [ 2.]
     [ 2.]
     [ 0.]
     [ 3.]
     [ 3.]
     [ 1.]
     [ 2.]
     [ 1.]
     [ 2.]
     [ 2.]
     [ 4.]
     [ 3.]
     [ 3.]
     [ 2.]
     [ 4.]
     [ 0.]
     [ 0.]
     [ 3.]
     [ 3.]
     [ 3.]
     [ 3.]
     [ 2.]
     [ 0.]
     [ 1.]
     [ 2.]
     [ 3.]
     [ 0.]
     [ 2.]
     [ 2.]
     [ 2.]
     [ 3.]
     [ 2.]
     [ 2.]
     [ 2.]
     [ 4.]
     [ 1.]
     [ 1.]
     [ 3.]
     [ 3.]
     [ 4.]
     [ 1.]
     [ 2.]
     [ 1.]
     [ 1.]
     [ 3.]
     [ 1.]
     [ 0.]
     [ 4.]
     [ 0.]
     [ 3.]
     [ 3.]
     [ 4.]
     [ 4.]
     [ 1.]
     [ 4.]
     [ 3.]
     [ 0.]
     [ 2.]]


ëª¨ë¸ì´ train setì—ì„œ ê½¤ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì¸ë‹¤! ì´ì œ test setì— ì ìš©í•´ë³´ì.

### 1.4 - test set performance 



```python
print("Training set:")
pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)
print('Test set:')
pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)
```

    Training set:
    Accuracy: 0.977272727273
    Test set:
    Accuracy: 0.857142857143


ë§Œì•½ ëœë¤ ì¶”ì¸¡ì„ í–ˆë‹¤ë©´ ì´ 5ê°œì˜ í´ë˜ìŠ¤ì´ê¸° ë•Œë¬¸ì— 20%ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ì„ ê²ƒì´ë‹¤. ì´ì— ë¹„í•´ ìœ„ì˜ ì •í™•ë„ëŠ” ë§¤ìš° ë†’ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, 127ê°œ ë°–ì— ì•ˆë˜ëŠ” ìƒ˜í”Œë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ì¶˜ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ìœ ëŠ” ì•„ë˜ ì˜ˆì‹œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.


```python
def print_predictions(X, pred):
    print()
    for i in range(X.shape[0]):
        print(X[i], label_to_emoji(int(pred[i])))
```


```python
X_my_sentences = np.array(["i adore you", "i love you" ,"funny lol", 'lets play a ball', "food is ready", "not feeling happy"])
Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])

pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)
print_predictions(X_my_sentences, pred)
```

    Accuracy: 0.833333333333
    
    i adore you â¤ï¸
    i love you â¤ï¸
    funny lol ğŸ˜„
    lets play a ball âš¾
    food is ready ğŸ´
    not feeling happy ğŸ˜„


ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, train setì—ëŠ” ì—†ë˜ 'adore'ê³¼ 'beloved'ë¼ëŠ” ë‹¨ì–´ì—ë„ 'love'ì˜ â¤ï¸ê°€ ë¶™ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” 'adore'ê³¼ 'beloved'ê°€ 'love'ì™€ ë¹„ìŠ·í•œ embeddingì„ ê°–ê³  ìˆì–´ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ì¼ë°˜í™”ë¥¼ ì‹œì¼°ê¸° ë•Œë¬¸ì´ë‹¤.

í•˜ì§€ë§Œ 'not feeling happy'ì—ì„œëŠ” ì•Œë§ì§€ ì•Šì€ ì´ëª¨í‹°ì½˜ì´ ë‚˜ì˜¤ëŠ” ê²ƒë„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ë‹¨ì–´ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¬¸ì œë‹¤. ì‹¤ì œë¡œ confusion matrixë¡œ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ì‚´í´ë³´ë©´ 3ë²ˆ ì´ëª¨í‹°ì½˜(ğŸ˜)ì´ ê°€ì¥ ë‚®ì€ ì˜ˆì¸¡ë„ë¥¼ ë³´ì¸ë‹¤.


```python
print(Y_test.shape)
print('           '+ label_to_emoji(0)+ '   ' + label_to_emoji(1) + '  ' +  label_to_emoji(2)+ '  ' + label_to_emoji(3)+'   ' + label_to_emoji(4))
print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))
plot_confusion_matrix(Y_test, pred_test)
```

    (56,)
               â¤ï¸   âš¾  ğŸ˜„  ğŸ˜   ğŸ´
    Predicted  0.0  1.0  2.0  3.0  4.0  All
    Actual                                 
    0            6    0    0    1    0    7
    1            0    8    0    0    0    8
    2            2    0   16    0    0   18
    3            1    1    2   12    0   16
    4            0    0    1    0    6    7
    All          9    9   19   13    6   56



![png](output_35_1.png)


ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¨ì–´ ìˆœì„œë¥¼ ê³ ë ¤í•œ ì •êµí•œ ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ì.

## 2 - Emojifier-V2: Using LSTM: 

ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ê³ ë ¤í•œ LSTM ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì. ì´ì „ ëª¨ë¸ì²˜ëŸ¼ pre-trainëœ ì›Œë“œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì´ë²ˆì—ëŠ” LSTMì— feedí•  ê²ƒì´ë‹¤. 


```python
import numpy as np
np.random.seed(0)
from keras.models import Model
from keras.layers import Dense, Input, Dropout, LSTM, Activation
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.initializers import glorot_uniform
np.random.seed(1)
```

    Using TensorFlow backend.


### 2.1 - Overview of the model

Emojifier-v2 êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<img src="images/emojifier-v2.png" style="width:700px;height:400px;"> <br>
<caption><center> **Figure 3**: Emojifier-V2. 2-layer LSTM ì‹œí€€ìŠ¤ ë¶„ë¥˜ê¸° </center></caption>



### 2.2 - The Embedding layer

Embedding ë ˆì´ì–´ë¥¼ êµ¬í˜„í•˜ì. ì´ ë ˆì´ì–´ëŠ” inputìœ¼ë¡œ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ, max ì¸í’‹ ê¸¸ì´) shapeì˜ í–‰ë ¬ì´ë‹¤. ì´ í–‰ë ¬ ê°’ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ëœ ë¬¸ì¥ë“¤ì„ ëŒ€í‘œí•œë‹¤. ì¦‰, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë³€í™˜ëœë‹¤.

<img src="images/embedding1.png" style="width:700px;height:250px;">
<caption><center> **Figure 4**: Embedding layer. ì´ ê·¸ë¦¼ì€ ë‘ ê°œì˜ ìƒ˜í”Œì´ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ê±°ì³ê°€ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ë‘˜ë‹¤ `max_len=5`ì— ë§ê²Œ zero-padding ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, ìµœì¢… ê°’ì˜ í¬ê¸°ê°€ (2,max_len,50)ì¸ ì´ìœ ëŠ” ì‚¬ìš©í•œ word embeddingì˜ í¬ê¸°ê°€ 50ì´ê¸° ë•Œë¬¸ì´ë‹¤.</center></caption>




```python
def sentences_to_indices(X, word_to_index, max_len):

    m = X.shape[0]                                   # train set ìƒ˜í”Œ ìˆ˜
    
    X_indices = np.zeros((m, max_len))
    
    for i in range(m):                               
        
        # ì†Œë¬¸ìë¡œ ë³€í™˜ í›„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° 
        sentence_words =X[i].lower().split()

        j = 0

        for w in sentence_words:
            X_indices[i, j] = word_to_index[w]
            j = j + 1

    return X_indices
```


```python
X1 = np.array(["funny lol", "lets play baseball", "food is ready for you"])
X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)
print("X1 =", X1)
print("X1_indices =", X1_indices)
```

    X1 = ['funny lol' 'lets play baseball' 'food is ready for you']
    X1_indices = [[ 155345.  225122.       0.       0.       0.]
     [ 220930.  286375.   69714.       0.       0.]
     [ 151204.  192973.  302254.  151349.  394475.]]


pre-trainëœ ë‹¨ì–´ ë²¡í„°ë¥¼ ì´ìš©í•´ Embedding ë ˆì´ì–´ë¥¼ êµ¬í˜„í•˜ì.


```python
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    
    vocab_len = len(word_to_index) + 1                 
    emb_dim = word_to_vec_map["cucumber"].shape[0]     

    emb_matrix = np.zeros((vocab_len, emb_dim))

    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]

    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)

    # Build the embedding layer
    embedding_layer.build((None,))
    
    # Set the weights of the embedding layer
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```


```python
embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
print("weights[0][1][3] =", embedding_layer.get_weights()[0][1][3])
```

    weights[0][1][3] = -0.3403


## 2.3 Building the Emojifier-V2

ì´ì œ Emojifier-V2 modelì„ êµ¬ì¶•í•´ë³´ì. ëª¨ë¸ êµ¬ì¡°ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.

<img src="images/emojifier-v2.png" style="width:700px;height:400px;"> <br>
<caption><center> **Figure 3**: Emojifier-v2. 2-layer LSTM ì‹œí€€ìŠ¤ ë¶„ë¥˜ê¸° </center></caption>


```python
def Emojify_V2(input_shape, word_to_vec_map, word_to_index):

    sentence_indices = Input(input_shape, dtype='int32')

    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)

    embeddings = embedding_layer(sentence_indices)    
    
    X = LSTM(128, return_sequences=True)(embeddings)
    X = Dropout(0.5)(X)
    X = LSTM(128, return_sequences=False)(X)
    X = Dropout(0.5)(X)
    X = Dense(5)(X)
    X = Activation('softmax')(X)

    model = Model(inputs=sentence_indices, outputs=X)
    
    return model
```


```python
model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         (None, 10)                0         
    _________________________________________________________________
    embedding_2 (Embedding)      (None, 10, 50)            20000050  
    _________________________________________________________________
    lstm_1 (LSTM)                (None, 10, 128)           91648     
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10, 128)           0         
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 128)               131584    
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 5)                 645       
    _________________________________________________________________
    activation_1 (Activation)    (None, 5)                 0         
    =================================================================
    Total params: 20,223,927
    Trainable params: 223,877
    Non-trainable params: 20,000,050
    _________________________________________________________________



```python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì. 


```python
X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)
Y_train_oh = convert_to_one_hot(Y_train, C = 5)
```


```python
model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)
```

    Epoch 1/50
    132/132 [==============================] - 0s - loss: 1.6084 - acc: 0.1970     
    Epoch 2/50
    132/132 [==============================] - 0s - loss: 1.5322 - acc: 0.2955     
    Epoch 3/50
    132/132 [==============================] - 0s - loss: 1.5008 - acc: 0.3258     
    Epoch 4/50
    132/132 [==============================] - 0s - loss: 1.4385 - acc: 0.3561     
    Epoch 5/50
    132/132 [==============================] - 0s - loss: 1.3470 - acc: 0.4545     
    Epoch 6/50
    132/132 [==============================] - 0s - loss: 1.2334 - acc: 0.5076     
    Epoch 7/50
    132/132 [==============================] - 0s - loss: 1.1763 - acc: 0.4470     
    Epoch 8/50
    132/132 [==============================] - 0s - loss: 1.0547 - acc: 0.5758     
    Epoch 9/50
    132/132 [==============================] - 0s - loss: 0.8771 - acc: 0.7121     
    Epoch 10/50
    132/132 [==============================] - 0s - loss: 0.8233 - acc: 0.6970     
    Epoch 11/50
    132/132 [==============================] - 0s - loss: 0.7029 - acc: 0.7500     
    Epoch 12/50
    132/132 [==============================] - 0s - loss: 0.6006 - acc: 0.8030     
    Epoch 13/50
    132/132 [==============================] - 0s - loss: 0.4931 - acc: 0.8333     
    Epoch 14/50
    132/132 [==============================] - 0s - loss: 0.5097 - acc: 0.8333     
    Epoch 15/50
    132/132 [==============================] - 0s - loss: 0.4788 - acc: 0.8258     
    Epoch 16/50
    132/132 [==============================] - 0s - loss: 0.3541 - acc: 0.8636     
    Epoch 17/50
    132/132 [==============================] - 0s - loss: 0.3901 - acc: 0.8636     
    Epoch 18/50
    132/132 [==============================] - 0s - loss: 0.6468 - acc: 0.8106     
    Epoch 19/50
    132/132 [==============================] - 0s - loss: 0.5181 - acc: 0.8182     
    Epoch 20/50
    132/132 [==============================] - 0s - loss: 0.3953 - acc: 0.8409     
    Epoch 21/50
    132/132 [==============================] - 0s - loss: 0.4731 - acc: 0.8182     
    Epoch 22/50
    132/132 [==============================] - 0s - loss: 0.3907 - acc: 0.8636     
    Epoch 23/50
    132/132 [==============================] - 0s - loss: 0.3810 - acc: 0.8561     
    Epoch 24/50
    132/132 [==============================] - 0s - loss: 0.3059 - acc: 0.9091     - ETA: 0s - loss: 0.2880 - acc: 0.91
    Epoch 25/50
    132/132 [==============================] - 0s - loss: 0.3507 - acc: 0.8864     
    Epoch 26/50
    132/132 [==============================] - 0s - loss: 0.2409 - acc: 0.9394     
    Epoch 27/50
    132/132 [==============================] - 0s - loss: 0.3188 - acc: 0.8864     
    Epoch 28/50
    132/132 [==============================] - 0s - loss: 0.2397 - acc: 0.9318     
    Epoch 29/50
    132/132 [==============================] - 0s - loss: 0.3943 - acc: 0.8712     
    Epoch 30/50
    132/132 [==============================] - 0s - loss: 0.2691 - acc: 0.9091     
    Epoch 31/50
    132/132 [==============================] - 0s - loss: 0.2948 - acc: 0.8864     
    Epoch 32/50
    132/132 [==============================] - 0s - loss: 0.2066 - acc: 0.9318     
    Epoch 33/50
    132/132 [==============================] - 0s - loss: 0.2132 - acc: 0.9470     
    Epoch 34/50
    132/132 [==============================] - 0s - loss: 0.1557 - acc: 0.9545     - ETA: 0s - loss: 0.1197 - acc: 0.96
    Epoch 35/50
    132/132 [==============================] - 0s - loss: 0.1638 - acc: 0.9621     
    Epoch 36/50
    132/132 [==============================] - 0s - loss: 0.1859 - acc: 0.9394     
    Epoch 37/50
    132/132 [==============================] - 0s - loss: 0.1709 - acc: 0.9470     
    Epoch 38/50
    132/132 [==============================] - 0s - loss: 0.2077 - acc: 0.9394     
    Epoch 39/50
    132/132 [==============================] - 0s - loss: 0.1371 - acc: 0.9621     
    Epoch 40/50
    132/132 [==============================] - 0s - loss: 0.1482 - acc: 0.9621     
    Epoch 41/50
    132/132 [==============================] - 0s - loss: 0.0864 - acc: 0.9848     
    Epoch 42/50
    132/132 [==============================] - 0s - loss: 0.0776 - acc: 0.9848     
    Epoch 43/50
    132/132 [==============================] - 0s - loss: 0.0785 - acc: 0.9848     
    Epoch 44/50
    132/132 [==============================] - 0s - loss: 0.0482 - acc: 0.9924     
    Epoch 45/50
    132/132 [==============================] - 0s - loss: 0.0809 - acc: 0.9848     
    Epoch 46/50
    132/132 [==============================] - 0s - loss: 0.1169 - acc: 0.9773     
    Epoch 47/50
    132/132 [==============================] - 0s - loss: 0.1552 - acc: 0.9470     
    Epoch 48/50
    132/132 [==============================] - 0s - loss: 0.2767 - acc: 0.9242     
    Epoch 49/50
    132/132 [==============================] - 0s - loss: 0.1178 - acc: 0.9773     
    Epoch 50/50
    132/132 [==============================] - 0s - loss: 0.1793 - acc: 0.9470     





    <keras.callbacks.History at 0x7f357ec2fc50>



Test setìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•´ë³´ì.


```python
X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)
Y_test_oh = convert_to_one_hot(Y_test, C = 5)
loss, acc = model.evaluate(X_test_indices, Y_test_oh)
print()
print("Test accuracy = ", acc)
```

    32/56 [================>.............] - ETA: 0s
    Test accuracy =  0.821428562914


ì•½ 82%ì˜ ì •í™•ë„ë¥¼ ë³´ì˜€ë‹¤! ì˜ëª» ì˜ˆì¸¡í•œ ê²ƒë“¤ì€ ë¬´ì—‡ì¼ê¹Œ? í•œë²ˆ ì‚´í´ë³´ì.


```python
# mislabelled examples
C = 5
y_test_oh = np.eye(C)[Y_test.reshape(-1)]
X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)
pred = model.predict(X_test_indices)
for i in range(len(X_test)):
    x = X_test_indices
    num = np.argmax(pred[i])
    if(num != Y_test[i]):
        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())
```

    Expected emoji:ğŸ˜„ prediction: she got me a nice present	â¤ï¸
    Expected emoji:ğŸ˜ prediction: work is hard	ğŸ˜„
    Expected emoji:ğŸ˜ prediction: This girl is messing with me	â¤ï¸
    Expected emoji:ğŸ´ prediction: any suggestions for dinner	ğŸ˜„
    Expected emoji:â¤ï¸ prediction: I love taking breaks	ğŸ˜
    Expected emoji:ğŸ˜„ prediction: you brighten my day	â¤ï¸
    Expected emoji:ğŸ˜„ prediction: will you be my valentine	â¤ï¸
    Expected emoji:ğŸ´ prediction: See you at the restaurant	ğŸ˜„
    Expected emoji:ğŸ˜ prediction: go away	âš¾
    Expected emoji:ğŸ´ prediction: I did not have breakfast â¤ï¸


ì²«ë²ˆì§¸ ëª¨ë¸ì—ì„œ 'not feeling happy'ê°€ ğŸ˜„ë¡œ ì˜ëª» ì˜ˆì¸¡ë˜ì—ˆë˜ ê²ƒì„ ë´¤ë‹¤. ê·¸ë ‡ë‹¤ë©´ LSTMì„ ì ìš©í•œ ëª¨ë¸ì€ ì–´ë–¨ê¹Œ? í™•ì¸í•´ë³´ì.


```python
x_test = np.array(['not feeling happy'])
X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)
print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))
```

    not feeling happy ğŸ˜


ì´ë²ˆì—ëŠ” ğŸ˜ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
